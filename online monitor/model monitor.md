前言
====
  GPT的火爆点燃了所有科技公司对于AI进一步探索和应用的决心，然而大模型的开发需要高昂的成本，但开源大模型现在越来越多，对开源大模型上进行微调
  fintune而后应用到公司的业务上，已经成为越来越多科技公司业务上的一个选择。
  
如何对模型评估
====
  作为一个QA，跟随时代的浪潮，业务测试也逐渐接触到大模型，如果对一个模型效果进行评估，也是我们要学习的，下面是本人的一点心得：
  线下：准确率&性能。  
  线上：用户满足度评估
  模型的线下准确率评估，一般我们是创建评估集->分析模型在评估集上的准确率、错误原因 ->模型优化 -> 分析模型在评估集上的准确率、错误原因 ->达到可上线标准 ->模型上线
  模型的性能评估，因为一般大模型的参数都是Billion级别的需要巨大的计算资源，所以大模型的成本昂贵，性能好不好也是模型评估的重要指标，这里的性能评估和我们普通业务测试是相似的，使用压测工具、脚本对模型的qps进行评估
  这里主要想说的是模型上线后的效果评估

正文
===
  1、模型上线后效果评估
  ---
 分享一下我自己的一个线上效果评估监控流程，我们的业务中使用的是一个分类模型，会将用户的请求进行分类，针对分类结果，再流转到各个子服务模块进行处理
 那么，作为QA，从用户的角度出发，一个新的模型上线后，期望是用户的使用效果达到提升<br>
 如何监控用户对模型的满足度，我们先梳理一下思路<br>
 *  对于用户来讲，`我对系统有一系列的输入`，系统内部处理，`对我有一个输出`
 *  我们要在线上获取用户的输入和系统的输出
 *  我们要使用一个工具评估用户的输入和系统的输出
 *  我们要将评估结果推送到群里，或者是保存在平台上，以便大家可以随时查阅
 
 
  

  
  
  
