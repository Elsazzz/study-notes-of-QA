前言
====
  GPT的火爆点燃了所有科技公司对于AI进一步探索和应用的决心，然而大模型的开发需要高昂的成本，但开源大模型现在越来越多，对开源大模型上进行微调
  fintune而后应用到公司的业务上，已经成为越来越多科技公司业务上的一个选择。
  
如何对模型评估
====
  作为一个QA，跟随时代的浪潮，业务测试也逐渐接触到大模型，如果对一个模型效果进行评估，也是我们要学习的，下面是本人的一点心得：
  *  线下：准确率&性能。
      *  模型的线下准确率评估，一般我们是创建评估集->分析模型在评估集上的准确率、错误原因 ->模型优化 -> 分析模型在评估集上的准确率、错误原因 ->达到可上线标准 ->模型上线
      *  模型的性能评估，因为一般大模型的参数都是Billion级别的需要巨大的计算资源，所以大模型的成本昂贵，性能好不好也是模型评估的重要指标，这里的性能评估和我们普通业务测试是相似的，使用压测工具、脚本对模型的qps进行评估

  *  线上：用户满足度评估。这里主要想说的是模型上线后的效果评估

正文
===
  1、模型上线后效果评估
  ---
   分享一下我自己的一个线上效果评估监控流程，我们的业务中使用的是一个分类模型，会将用户的请求进行分类，针对分类结果，再流转到各个子服务模块进行处理
   那么，作为QA，从用户的角度出发，一个新的模型上线后，期望是用户的使用效果达到提升
   如何监控用户对模型的满足度，我们先梳理一下思路
 *  对于用户来讲，`我对系统有一系列的输入`，系统内部处理，`对我有一个输出`
 *  我们要在线上获取用户的输入和系统的输出
 *  我们要使用一个工具评估用户的输入和系统的输出
 *  我们要将评估结果推送到群里，或者是保存在平台上，以便大家可以随时查阅
 *
 2、根据思路，开始动手
  ---
  ### 从线上log中获取用户输入和输出
    grep ‘“input”.*"output"’ 20240102yourlog.log |shuf -n 1000 > 20240102eva.log
    线上log一般是备份到一个物理机中，分小时存储，一般会存储最近几天的，这里我们通过grep命令来获取当天的某个小时的log
    随机获取1000条进行评估
  ### 将获取的log推送到QA的开发机器上
  配置机器和QA开发机器的免密权限，[https://blog.csdn.net/smillqiang/article/details/126490492]
  scp 20240102eva.log yourmachineip:root
  ###  以上两个步骤设置定时任务
  crontab -e
  * 15 * * * cd xxx Command 1
  * 16 * * * Command 2
  这里我们是每天都跑一下评估
  ###  使用脚本解析log
  使用python脚本从我们的log中解析出用户的输入和系统输出字段
  ###  使用工具来评估用户满意度
  这里可以使用LLM来辅助我们聘雇
  ###  统计满意度结果，定期推送到群里
  设置crontab任务，调用推送消息接口，将每日评估结果发送到业务群中
  
      
 
  

  
  
  
